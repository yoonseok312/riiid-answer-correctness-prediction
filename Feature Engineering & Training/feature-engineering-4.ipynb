{"cells":[{"metadata":{},"cell_type":"markdown","source":"# *DATA PREPARATION*"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport sys\nimport time\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport gc\nimport random\nfrom pickle import dump, load","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_DTYPES = {\n    'row_id': np.uint64,\n    'timestamp': np.int64,\n    'user_id': np.uint64,\n    'content_id': np.uint16,\n    'content_type_id': np.int8,\n    'task_container_id': np.uint16,\n    'user_answer': np.int8,\n    'answered_correctly': np.int8,\n    'prior_question_elapsed_time': np.float32,\n    'prior_question_had_explanation': 'boolean'\n}\n\nQUESTION_DTYPES = {\n    'question_id': np.uint16,\n    'bundle_id': np.uint16,\n    'correct_answer': np.int8,\n    'part': np.int8,\n    'tags': str\n}\n\nLECTURE_DTYPES = {\n    'lecture_id': np.uint16,\n    'tag': np.uint16,\n    'part': np.int8,\n    'type_of':str\n}\n\nT_MASK = {24: 0, 23: 1, 58: 2, 134: 3, 52: 4, 124: 5, 44: 6, 123: 7, 67: 8, 167: 9, 161: 10, 43: 11, 80: 12,\n          46: 13, 28: 14, 103: 15, 94: 16, 186: 17, 26: 18, 180: 19, 50: 20, 182: 21, 31: 22, 6: 23, 15: 24,\n          11: 25, 108: 26, 47: 27, 76: 28, 165: 29, 174: 30, 48: 31, 152: 32, 132: 33, 170: 34, 49: 35,\n          181: 36, 159: 37, 145: 38, 73: 39, 64: 40, 1: 41, 7: 42, 16: 43, 57: 44, 21: 45, 95: 46, 72: 47,\n          91: 48, 125: 49, 157: 50, 96: 51, 156: 52, 53: 53, 55: 54, 45: 55, 4: 56, 133: 57, 136: 58, 75: 59,\n          39: 60, 89: 61, 65: 62, 117: 63, 173: 64, 83: 65, 8: 66, 166: 67, 25: 68, 168: 69, 79: 70, 3: 71,\n          97: 72, 60: 73, 128: 74, 179: 75, 14: 76, 151: 77, 164: 78, 112: 79, 116: 80, 42: 81, 22: 82, 0: 83,\n          127: 84, 160: 85, 147: 86, 19: 87, 32: 88, 183: 89, 12: 90, 9: 91, 86: 92, 109: 93, 175: 94, 10: 95,\n          115: 96, 78: 97, 171: 98, 148: 99, 113: 100, 27: 101, 35: 102, 169: 103, 92: 104, 122: 105, 54: 106,\n          114: 107, 18: 108, 17: 109, 56: 110, 107: 111, 90: 112, 163: 113, 126: 114, 29: 115, 66: 116,\n          106: 117, 135: 118, 2: 119, 87: 120, 138: 121, 71: 122, 100: 123, 41: 124, 30: 125, 154: 126,\n          102: 127, 84: 128, 81: 129, 37: 130, 146: 131, 185: 132, 155: 133, 176: 134, 143: 135, 121: 136,\n          85: 137, 162: 138, 184: 139, 104: 140, 38: 141, 140: 142, 82: 143, 120: 144, 20: 145, 88: 146,\n          141: 147, 119: 148, 139: 149, 150: 150, 98: 151, 62: 152, 33: 153, 144: 154, 158: 155, 74: 156,\n          13: 157, 61: 158, 110: 159, 69: 160, 137: 161, 111: 162, 34: 163, 118: 164, 153: 165, 129: 166,\n          178: 167, 105: 168, 177: 169, 36: 170, 172: 171, 142: 172, 63: 173, 101: 174, 59: 175, 5: 176,\n          131: 177, 99: 178, 93: 179, 51: 180, 77: 181, 40: 182, 70: 183, 149: 184, 68: 185, 187: 186, 130: 187}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_csv(file_name = \"train.csv\", dtype = None, skiprows = None, nrows = None, usecols = None):\n    data = pd.read_csv(file_name, dtype=dtype, skiprows = skiprows, nrows = nrows, low_memory = True, header = 0, usecols = usecols)\n    return data\n\ndef read_feather(file_name = \"../input/feather-data/train.feather\"):\n    data = pd.read_feather(file_name)\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Get ts_delta of train.csv, save to feather\ndef get_ts_delta():\n    df = read_feather()\n    df = df[df['content_type_id'] == 0]\n    tsdf = df['timestamp'].astype(np.int64)\n    del df\n    oidx = list(range(tsdf.shape[0]))\n    last = [oidx[-1]]\n    oidx = oidx[:-1]\n    last.extend(oidx)\n    del oidx\n    gc.collect()\n    tsdf.reset_index(drop = True, inplace = True)\n    retsdf = tsdf.reindex(index=last)\n    retsdf.reset_index(drop = True, inplace = True)\n    delta_tsdf = tsdf - retsdf\n    delta_tsdf[delta_tsdf < 0] = -1\n    del tsdf\n    del retsdf\n    gc.collect()\n    delta_tsdf = pd.DataFrame(delta_tsdf, dtype = np.int64)\n    delta_tsdf.rename(columns = {'timestamp': 'ts_delta'}, inplace = True)\n    retsdf = delta_tsdf\n    while delta_tsdf[delta_tsdf == 0].notna().max()['ts_delta']:\n        retsdf = retsdf.reindex(index=last)\n        retsdf.reset_index(drop = True, inplace = True)\n        delta_tsdf[delta_tsdf == 0] = retsdf[delta_tsdf == 0]\n    delta_tsdf.to_feather('ts_delta.feather')\n    return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Load train.csv and (scale ts and uid), get cor as label\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\ndef ts_delta_feature():\n    df = read_feather('../input/feather-data/ts_delta.feather')\n    df1 = df//60000\n    df1[df1 > 1440] = 1440\n    df1 = (df1 + 2).astype(np.int16)\n    df1.rename(columns = {'ts_delta': 'minute_delta'}, inplace = True)\n    df3 = (df//86400000)\n    df3[df3 > 30] = 30\n    df3 = (df3 + 2).astype(np.int8)\n    df3.rename(columns = {'ts_delta': 'day_delta'}, inplace = True)\n    df2 = df//2592000000\n    df2[df2 > 6] = 6\n    df2 = (df2 + 2).astype(np.int8)\n    df2.rename(columns = {'ts_delta': 'month_delta'}, inplace = True)\n    df1 = pd.concat([df1, df3, df2], axis = 1)\n    return df1\n\ndef load_train(skiprows= None, nrows = None):\n    df = read_feather(\"../input/feather-data/train.feather\")\n    df = df[df['content_type_id'] == 0]\n    gc.collect()\n    df.reset_index(drop = True, inplace = True)\n    df.drop(columns = ['user_answer', 'content_type_id'], inplace = True)#'timestamp'\n    df['prior_question_had_explanation'].fillna(0, inplace = True)\n    df['prior_question_had_explanation'] = df['prior_question_had_explanation'].astype(np.int8)\n    df['prior_question_elapsed_time'].fillna(23000, inplace = True) #mean: 25300 med: 21000\n    df['prior_question_elapsed_time'] = (df['prior_question_elapsed_time'] // 1000).astype(np.int16)\n    for x in df:\n        if (x == 'content_id' or x == 'task_container_id' or \n            x == 'prior_question_elapsed_time' or x == 'prior_question_had_explanation'):\n            df[x] = df[x] + 1\n        else: continue\n    label_df = df.pop('answered_correctly')\n    row_id = pd.DataFrame(df.pop('row_id'))\n    tsdf = ts_delta_feature()\n    df = pd.concat([row_id, tsdf, df], axis = 1)\n    df.to_feather('train_scaled.feather')\n    pd.DataFrame(label_df).to_feather('label_nolec.feather')\n    return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Load questions.csv and ohe part, preprocess tags\nfrom sklearn.preprocessing import LabelBinarizer\ndef load_q_content():\n    df = read_csv(file_name = '../input/riiid-test-answer-prediction/questions.csv', dtype = QUESTION_DTYPES)\n    df.drop(columns = 'correct_answer', inplace = True)\n    bundle_encoder = LabelEncoder()\n    df.bundle_id = bundle_encoder.fit_transform(df.bundle_id) + 1\n    df.question_id = df.question_id + 1\n    part_encoder = load(open('../input/feather-data/part_enc.pkl', 'rb'))\n    df.fillna(value = '0', inplace = True)\n    tag_content = df.pop('tags')\n    part_content = df.pop('part')\n    df['p_listen'] = (part_content < 5)\n    df['p_read'] = (part_content >= 5)\n    df['p_listen'] = df['p_listen'].astype(np.int8)\n    df['p_read'] = df['p_read'].astype(np.int8)\n    df['3_mul'] = part_content == 2\n    df['4_mul'] = part_content != 2\n    df['3_mul'] = df['3_mul'].astype(np.int8)\n    df['4_mul'] = df['4_mul'].astype(np.int8)\n    part_ohe = part_encoder.transform(part_content)\n    part_ohe = pd.DataFrame(part_ohe, dtype = np.int8, columns = ['p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7'])\n    df = pd.concat([df, part_ohe], axis = 1)\n    parsed_tag = []\n    for i in range(tag_content.shape[0]):\n        value = tag_content[i].split()\n        for j, k in enumerate(value):  \n            value[j] = int(T_MASK[int(k)])+1\n        value.sort(reverse = True)\n        pad = 6 - len(value)\n        value.extend([1]*pad)\n        parsed_tag.append(value)\n    parsed_tag = pd.DataFrame(parsed_tag, dtype = np.uint8, columns = ['t1', 't2', 't3', 't4', 't5', 't6'])\n    parsed_tag.drop(['t4', 't5', 't6'], axis = 1, inplace = True)\n    df = pd.concat([df, parsed_tag], axis = 1)\n    df.to_feather('questions_processed.feather')\n    return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Load lectures.csv and ohe type_of\nfrom sklearn.preprocessing import LabelBinarizer\ndef load_lecture():\n    df = read_csv(file_name = \"../input/riiid-test-answer-prediction/lectures.csv\", dtype = LECTURE_DTYPES)\n    tag_content = df.pop('tag')\n    parsed_tag = []\n    for i in range(tag_content.shape[0]):\n        value = [int(T_MASK[int(tag_content[i])])+1]\n        parsed_tag.append(value)\n    parsed_tag = pd.DataFrame(parsed_tag, dtype = np.uint8, columns = ['tag'])\n    type_encoder = load(open('../input/feather-data/lecty_enc.pkl', 'rb'))\n    type_ohe = type_encoder.transform(df['type_of'])\n    type_ohe = pd.DataFrame(type_ohe, dtype = np.int8, columns = ['ty1', 'ty2', 'ty3', 'ty4'])\n    df = pd.concat([df, parsed_tag, type_ohe], axis = 1)\n    df.drop(columns = ['type_of'], inplace = True)\n    df.to_feather('lectures_processed.feather')\n    return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Split train data into 7 folds, group by uid\nfrom sklearn.model_selection import GroupKFold\ndef split_data(df=None):\n    if df is None: df = read_feather('train_scaled.feather')\n    group_kfold = GroupKFold(n_splits=7)\n    groups = df['user_id']\n    x_df = df['row_id']\n    y_df = read_feather('label_nolec.feather')\n    y_df = y_df[y_df['answered_correctly'] != -1]\n    y_df = y_df['answered_correctly']\n    i = 0\n    for _, test_index in group_kfold.split(x_df, y_df, groups):\n        new_df = df.iloc[test_index]\n        new_df.reset_index(drop = True, inplace = True)\n        new_df.to_feather('train_scaled_{}.feather'.format(i))\n        new_label = y_df.iloc[test_index]\n        new_label.reset_index(drop = True, inplace = True)\n        pd.DataFrame(new_label).to_feather('label_data_{}.feather'.format(i))\n        i+=1\n    return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Create lecture dict of users n train.csv\ndef get_train_lecture():\n    df = read_feather()\n    df = df[df['content_type_id'] == 1]\n    gc.collect()\n    df.drop(columns = ['prior_question_elapsed_time', 'prior_question_had_explanation', \n                       'content_type_id', 'task_container_id', 'user_answer', 'answered_correctly'],\n            inplace = True)\n    df.reset_index(drop = True, inplace = True)\n    return df\n\ndef create_trainlec_dict():\n    df_trainlec = get_train_lecture()\n    df_lec = read_feather('lectures_processed.feather')\n    lec_pos = {}\n    for i in range(df_lec.shape[0]):\n        lec_pos[df_lec['lecture_id'][i]] = i\n    trainlec_dict = {}\n    for i in tqdm(range(df_trainlec.shape[0])):\n        uid = df_trainlec['user_id'][i]\n        if not uid in trainlec_dict: trainlec_dict[uid] = {}\n        lid = df_trainlec['content_id'][i]\n        pos = lec_pos[lid]\n        part = df_lec['part'][pos]\n        tag = df_lec['tag'][pos]\n        ltype = np.argmax(df_lec[['ty1', 'ty2', 'ty3', 'ty4']][pos : pos+1].to_numpy())\n        if not part in trainlec_dict[uid]: trainlec_dict[uid][part] = {'tt':[], 'lt':[], 'ts':[], 'rid':[]}\n        trainlec_dict[uid][part]['tt'].append(tag)\n        trainlec_dict[uid][part]['lt'].append(ltype)\n        trainlec_dict[uid][part]['ts'].append(df_trainlec['timestamp'][i])\n        trainlec_dict[uid][part]['rid'].append(df_trainlec['row_id'][i])\n    dump(trainlec_dict, open('trainlec_dict.pkl', 'wb')) # 149606 uid\n    return trainlec_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Concat train data with lecture data for each question.\nfrom bisect import bisect\ndef get_train_q(fold):\n    df = read_feather('../input/riiid-data-processing/train_scaled_{}.feather'.format(fold))\n    print(df.shape)\n    gc.collect()\n    df.reset_index(drop = True, inplace = True)\n    return df\n\ndef create_lecture_dep(fold):\n    part_df = read_csv(file_name = '../input/riiid-test-answer-prediction/questions.csv', dtype = QUESTION_DTYPES)\n    part_df.drop(columns = 'bundle_id', inplace=True)\n    part_df.drop(columns = 'correct_answer', inplace = True)\n    part_df.drop(columns = 'tags', inplace = True)\n    gc.collect()\n    #col: row_id  timestamp  user_id  content_id  content_type_id  task_container_id  prior_question_elapsed_time  prior_question_had_explanation\n    tdf = get_train_q(fold)\n    lect_dep_df = pd.DataFrame(columns = ['numlect', 'llecty1', 'llecty2', 'llecty3', 'llecty4', \n                                          'hour_past'], #tag of lecture not use\n                              index = range(tdf.shape[0]))\n    trainlec_dict = load(open('../input/feather-data/trainlec_dict.pkl', 'rb'))\n    user_range = {}\n    tpast = [[0,0,0,0]]*7\n    num_lect = [1]*7\n    prev_idx_cur = [-1]*7\n    with_df = []\n    for i in tqdm(range(tdf.shape[0])):\n        if i-1 >= 0 and tdf['task_container_id'][i] == tdf['task_container_id'][i-1]:\n            with_df.append(2)\n        elif i+1 < tdf.shape[0] and tdf['task_container_id'][i] == tdf['task_container_id'][i+1]:\n            with_df.append(2)\n        else: with_df.append(1)\n        uid = tdf['user_id'][i]\n        if not uid in user_range:\n            user_range[uid] = [tdf['row_id'][i], tdf['row_id'][i]]\n            tpast = [[0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0], [0,0,0,0]]\n            prev_idx_cur = [-1]*7\n            num_lect = [1]*7\n        user_range[uid][1] = tdf['row_id'][i]\n        if not uid in trainlec_dict: continue\n        cid = tdf['content_id'][i] - 1\n        assert part_df['question_id'][cid] == cid, 'hmm somethings wrong' + str(cid)\n        part = part_df['part'][cid]\n        if not part in trainlec_dict[uid]: continue\n        ts_cur = tdf['timestamp'][i]\n        idx_cur = bisect(trainlec_dict[uid][part]['ts'], ts_cur) - 1\n        if idx_cur == -1: continue\n        if idx_cur != prev_idx_cur[part-1]:\n            numlec_between = idx_cur - prev_idx_cur[part-1]\n            num_lect[part - 1] += numlec_between\n            for k in range(numlec_between):\n                t_incre = trainlec_dict[uid][part]['lt'][idx_cur - k]\n                tpast[part-1][t_incre] += 1\n        tlast = [0]*4\n        tlast[trainlec_dict[uid][part]['lt'][idx_cur]] = 1\n        taglast = trainlec_dict[uid][part]['tt'][idx_cur]\n        tsago = min((ts_cur - trainlec_dict[uid][part]['ts'][idx_cur])//3600000 + 2, 723)\n#         row_to_add = tpast[part-1].copy()\n        row_to_add = [num_lect[part - 1]]\n        row_to_add.extend(tlast)\n#         row_to_add.append(taglast)\n        row_to_add.append(tsago)\n        lect_dep_df.loc[i] = row_to_add\n        prev_idx_cur[part-1] = idx_cur\n    lect_dep_df['hour_past'].fillna(1, inplace = True)\n#     lect_dep_df['llectag'].fillna(1, inplace = True)\n    lect_dep_df['numlect'].fillna(1, inplace = True)\n    lect_dep_df.fillna(0, inplace = True)\n    for x in lect_dep_df:\n        if x == 'hour_past': lect_dep_df[x] = lect_dep_df[x].astype(np.uint16); continue\n        lect_dep_df[x] = lect_dep_df[x].astype(np.uint8)\n    with_df = pd.DataFrame(with_df, columns = ['with'], dtype = np.int8)\n    tdf = pd.concat([tdf, with_df, lect_dep_df], axis = 1)\n    tdf.to_feather('train_processed_{}.feather'.format(fold))\n#     dump(user_range, open('train_data/urange{}.pkl'.format(fold), 'wb'))\n    return tdf, lect_dep_df, user_range\n\ndef create_train_lec_dep_all(x):\n    for i in x:\n        df, df_to_add, urange = create_lecture_dep(i)\n        del df\n        del df_to_add\n        del urange\n        gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"##Merge train and questions data\ndef merge_train_and_q(fold):\n    qdf = read_feather('questions_processed.feather')\n    tdf = read_feather('../input/riiid-data-processing2/train_processed_{}.feather'.format(fold))\n    tdf = tdf.merge(qdf, left_on = 'content_id', right_on = 'question_id', how = 'left')\n    gc.collect()\n    a = tdf.pop('question_id')\n    a = tdf.pop('row_id')\n    del a\n    gc.collect()\n    tdf.to_feather('train_dat_{}.feather'.format(fold))\n    return 0\n\ndef merge_train_q_all(x):\n    for i in x:\n        merge_train_and_q(i)\n        gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def merge_train_label(x):\n    for i in x:\n        tdf = read_feather('train_dat_{}.feather'.format(i))\n        ldf = read_feather('../input/riiid-data-processing/label_data_{}.feather'.format(i))\n        tdf = pd.concat([tdf, ldf], axis = 1)\n        tdf.to_feather('train_dat_{}_plus.feather'.format(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def merge_all_dat():\n    all_train_dat = []\n    for x in range(7):\n        df = read_feather('train_dat_{}_plus.feather'.format(x))\n        all_train_dat.append(df)\n    df = pd.concat(all_train_dat, axis = 0, ignore_index = True)\n    for i in all_train_dat: del i\n    del all_train_dat\n    df.to_feather('all_train_dat_plus.feather')\n    del df\n    return 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_user_dict():\n    tdf = read_feather('all_train_dat_plus.feather')\n    tdf = tdf.groupby('user_id')\n    user_dict = tdf.groups\n    for k in tqdm(user_dict):\n        user_dict[k] = list(user_dict[k])\n    dump(user_dict, open('user_dict.pkl', 'wb'))\n    return 0","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# get_ts_delta()\n# load_train()\nload_q_content()\n# load_lecture()\n# split_data()\n# create_trainlec_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = [0,1,2,3,4,5,6]\n# create_train_lec_dep_all(x)\nmerge_train_q_all(x)\nmerge_train_label(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"merge_all_dat()\nget_user_dict()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}